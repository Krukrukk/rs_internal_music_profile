{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Callable, Any\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Linear,\n",
    "    ReLU,\n",
    "    TripletMarginLoss,\n",
    "    TripletMarginWithDistanceLoss,\n",
    "    PairwiseDistance,\n",
    ")\n",
    "from torch.nn.functional import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model(\n",
    "    **bert_tokenizer(\n",
    "        \"przykładowy tekst w języku polskim\", return_tensors=\"pt\", padding=True\n",
    "    )\n",
    ").last_hidden_state.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset:\n",
    "\n",
    "- Positive example is a song paired with mean vector of a playlist songs, the song belongs to\n",
    "- Negative example is a song paired with mean vector of a playlist songs, the song doesn't belong to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_in_one_playlist_dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = [\"track_name\", \"artist_name\"]\n",
    "numeric_features: dict = {\n",
    "    \"duration_ms\": {\n",
    "        \"min_val\": 0,\n",
    "        \"max_val\": 6950000,\n",
    "        \"desc\": \"duration of a song in ms\",\n",
    "    },\n",
    "    \"danceability\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\",\n",
    "    },\n",
    "    \"energy\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\",\n",
    "    },\n",
    "    \"key\": {\n",
    "        \"min_val\": -1,\n",
    "        \"max_val\": 11,\n",
    "        \"desc\": \"The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\",\n",
    "    },\n",
    "    \"loudness\": {\n",
    "        \"min_val\": -60.0,\n",
    "        \"max_val\": 0.0,\n",
    "        \"desc\": \"The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\",\n",
    "    },\n",
    "    \"mode\": {\n",
    "        \"min_val\": 0,\n",
    "        \"max_val\": 1,\n",
    "        \"desc\": \"Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\",\n",
    "    },\n",
    "    \"speechiness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\",\n",
    "    },\n",
    "    \"acousticness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\",\n",
    "    },\n",
    "    \"instrumentalness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Predicts whether a track contains no vocals. 'Ooh' and 'aah' sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly 'vocal'. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\",\n",
    "    },\n",
    "    \"liveness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\",\n",
    "    },\n",
    "    \"valence\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\",\n",
    "    },\n",
    "}\n",
    "\n",
    "text_features = [\"track_name\", \"artist_name\", \"album_name\"]\n",
    "\n",
    "\n",
    "def preprocess_features(\n",
    "    df: pd.DataFrame, numeric_features: dict[str, dict[str, Any]], section_feature: str\n",
    ") -> pd.DataFrame:\n",
    "    stand_data = {}\n",
    "\n",
    "    # Process numeric features\n",
    "    for key in numeric_features:\n",
    "        # normalize data\n",
    "        df[key] = (df[key] - numeric_features[key][\"min_val\"]) / (\n",
    "            numeric_features[key][\"max_val\"] - numeric_features[key][\"min_val\"]\n",
    "        )\n",
    "        stand_data[key] = {\n",
    "            \"mean\": df[key].mean(),\n",
    "            \"std\": df[key].std(),\n",
    "            \"max\": numeric_features[key][\"max_val\"],\n",
    "            \"min\": numeric_features[key][\"min_val\"],\n",
    "        }\n",
    "        # # standarize data\n",
    "        # df[key] = (df[key] - stand_data[key][\"mean\"]) / stand_data[key][\"std\"]\n",
    "\n",
    "    # Process sections data\n",
    "    seq_col = f\"{section_feature}_seq\"\n",
    "    df[seq_col] = df.analysis_sections.apply(\n",
    "        lambda section: torch.tensor(\n",
    "            [item[section_feature] for item in (section if section is not None else [])]\n",
    "        )\n",
    "    )\n",
    "    # Normalize/standarize data\n",
    "    all_tempo = torch.tensor([item for row in df[seq_col] for item in row])\n",
    "\n",
    "    max_tempo = all_tempo.max()\n",
    "    min_tempo = all_tempo.min()\n",
    "    all_tempo = (all_tempo - min_tempo) / (max_tempo - min_tempo)\n",
    "    stand_data[seq_col] = {\n",
    "        \"mean\": all_tempo.mean(),\n",
    "        \"std\": all_tempo.std(),\n",
    "        \"max\": max_tempo,\n",
    "        \"min\": min_tempo,\n",
    "    }\n",
    "\n",
    "    df[seq_col] = df[seq_col].apply(\n",
    "        lambda row: torch.tensor(\n",
    "            [((x - min_tempo) / (max_tempo - min_tempo)) * 2 - 1 for x in row]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # pad to 14\n",
    "    df[seq_col] = df[seq_col].apply(lambda x: x[:14])\n",
    "    df[seq_col] = df[seq_col].apply(\n",
    "        lambda x: torch.cat(\n",
    "            (\n",
    "                x,\n",
    "                torch.tensor([0] * (14 - len(x))),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return stand_data\n",
    "\n",
    "\n",
    "stand_data = preprocess_features(df, numeric_features, \"tempo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "train_split = 0.8\n",
    "playlists = list(df.id_playlist.unique())\n",
    "total_len = len(playlists)\n",
    "random.shuffle(playlists)\n",
    "train_playlists = set(playlists[: int(0.8 * total_len)])\n",
    "test_playlists = set(playlists[len(train_playlists) :])\n",
    "\n",
    "df_train = df.query(\"id_playlist in @train_playlists\")\n",
    "df_test = df.query(\"id_playlist in @test_playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlist = df[df.id_playlist == 2]\n",
    "# torch.tensor(playlist[list(numeric_features.keys())].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_playlist', 'name', 'collaborative', 'pid', 'modified_at',\n",
       "       'num_tracks', 'num_albums', 'num_followers', 'num_edits', 'num_artists',\n",
       "       'pos', 'artist_name', 'track_uri', 'artist_uri', 'track_name',\n",
       "       'album_uri', 'duration_ms', 'album_name', 'danceability', 'energy',\n",
       "       'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
       "       'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature',\n",
       "       'num_samples', 'duration', 'offset_seconds', 'window_seconds',\n",
       "       'analysis_sample_rate', 'analysis_channels', 'end_of_fade_in',\n",
       "       'start_of_fade_out', 'tempo_confidence', 'time_signature_confidence',\n",
       "       'key_confidence', 'mode_confidence', 'analysis_sections', 'tempo_seq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpotifyDataset(Dataset):\n",
    "    \"\"\"Wrapper dataset that draws triplets from the Spotify dataset.\n",
    "\n",
    "    size: arbitrary 'length' of dataset, number of triplets to draw in one epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame: pd.DataFrame,\n",
    "        model: BertModel,\n",
    "        tokenizer: BertTokenizer,\n",
    "        seq_feature: str = \"\",\n",
    "        size: int = 10000,\n",
    "        numeric_features: dict = [],\n",
    "        text_features: list = [],\n",
    "        data_path: str = \"data/embedds\",\n",
    "        prefix: str = \"spotify\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super(SpotifyDataset, self).__init__()\n",
    "        self.df = frame\n",
    "        self.playlists = list(frame.id_playlist.unique())\n",
    "        self.numeric_features = numeric_features\n",
    "        self.text_features = text_features\n",
    "        self.seq_feature = seq_feature\n",
    "        self.size = size\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.numeric_keys = list(numeric_features.keys())\n",
    "        self.data_path = data_path\n",
    "        self.prefix = prefix\n",
    "        self.device = device\n",
    "        self.playlist_bags = self.prepare_bags()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.get_random_triplet()\n",
    "\n",
    "    def _prepare(self):\n",
    "        _ensure_exists(self.data_path)\n",
    "        for _, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    self.data_path, f\"{self.prefix}_{row['track_uri']}.pckl\"\n",
    "                ).replace(\":\", \"_\"),\n",
    "                \"wb\",\n",
    "            ) as file:\n",
    "                pickle.dump(\n",
    "                    self.model(\n",
    "                        **self.tokenizer(\n",
    "                            list(row[self.text_features]),\n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=True,\n",
    "                        ).to(self.device)\n",
    "                    )\n",
    "                    .last_hidden_state.mean(dim=1)\n",
    "                    .flatten(),\n",
    "                    file,\n",
    "                )\n",
    "\n",
    "    def read_uri(self, uri):\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                self.data_path,\n",
    "                f\"{self.prefix}_{uri}.pckl\".replace(\":\", \"_\"),\n",
    "            ),\n",
    "            \"rb\",\n",
    "        ) as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "    def prepare_bags(self):\n",
    "        bags = {}\n",
    "        print(\"Prepairing playlists\")\n",
    "        for _id in tqdm(self.playlists):\n",
    "            playlist = self.df[self.df.id_playlist == _id]\n",
    "            playlist_embedd = torch.stack(\n",
    "                [self.read_uri(row[\"track_uri\"]) for i, row in playlist.iterrows()]\n",
    "            ).mean(axis=0)\n",
    "\n",
    "            bags[_id] = {\n",
    "                \"data\": playlist,\n",
    "                \"features\": torch.tensor(playlist[self.numeric_keys].mean()).float(),\n",
    "                \"embedds\": playlist_embedd.float(),\n",
    "                \"seq\": torch.stack(list(playlist[self.seq_feature]))\n",
    "                .float()\n",
    "                .mean(axis=0)\n",
    "                .reshape(-1, 1),\n",
    "            }\n",
    "        return bags\n",
    "\n",
    "    def get_random_triplet(self):\n",
    "        # get a random song and its playlist\n",
    "        anchor_song = self.df.sample(1).iloc[0]\n",
    "        anchor_playlist_id: int = anchor_song.id_playlist\n",
    "\n",
    "        # positive eanchor_songxample\n",
    "        positive_playlist_id: int = anchor_playlist_id\n",
    "\n",
    "        # negative example -> get a playlist song does't belong to. Stupid solution\n",
    "        while True:\n",
    "            negative_playlist_id = random.choice(self.playlists)\n",
    "            # check if it is a different playlist\n",
    "            if negative_playlist_id == anchor_playlist_id:\n",
    "                continue  # find another oneds\n",
    "            # check if it doesn't have our song\n",
    "            if len(\n",
    "                self.df[\n",
    "                    (self.df.id_playlist == negative_playlist_id)\n",
    "                    & (self.df.track_uri == anchor_song.track_uri)\n",
    "                ]\n",
    "            ):\n",
    "                continue  # find another one\n",
    "            break\n",
    "\n",
    "        # positive and negative playlist data\n",
    "        positive_playlist = self.playlist_bags[positive_playlist_id][\"data\"]\n",
    "        negative_playlist = self.playlist_bags[negative_playlist_id][\"data\"]\n",
    "\n",
    "        # get anchor numeric features\n",
    "        anchor_song_features = torch.tensor(anchor_song[self.numeric_keys]).float()\n",
    "\n",
    "        # get positive and negative playlist numeric features\n",
    "        positive_playlist_features = self.playlist_bags[positive_playlist_id][\n",
    "            \"features\"\n",
    "        ]\n",
    "        negative_playlist_features = self.playlist_bags[negative_playlist_id][\n",
    "            \"features\"\n",
    "        ]\n",
    "\n",
    "        # get anchor text features\n",
    "        anchor_song_embedds = self.read_uri(anchor_song[\"track_uri\"]).float()\n",
    "\n",
    "        # get positive and negative playlist text features\n",
    "\n",
    "        positive_playlist_embedds = self.playlist_bags[positive_playlist_id][\"embedds\"]\n",
    "        negative_playlist_embedds = self.playlist_bags[negative_playlist_id][\"embedds\"]\n",
    "\n",
    "        # get anchor sequence features\n",
    "        anchor_song_seq = anchor_song[self.seq_feature].reshape(-1, 1).float()\n",
    "\n",
    "        # get positive/negative playlist seq features\n",
    "        positive_playlist_seq = self.playlist_bags[positive_playlist_id][\"seq\"]\n",
    "        negative_playlist_seq = self.playlist_bags[negative_playlist_id][\"seq\"]\n",
    "        return (\n",
    "            (\n",
    "                anchor_song_features,\n",
    "                anchor_song_embedds,\n",
    "                anchor_song_seq,\n",
    "            ),\n",
    "            (\n",
    "                positive_playlist_features,\n",
    "                positive_playlist_embedds,\n",
    "                positive_playlist_seq,\n",
    "            ),\n",
    "            (\n",
    "                negative_playlist_features,\n",
    "                negative_playlist_embedds,\n",
    "                negative_playlist_seq,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing playlists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2674/2674 [00:44<00:00, 59.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing playlists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 669/669 [00:10<00:00, 61.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = SpotifyDataset(\n",
    "    frame=df_train,\n",
    "    model=bert_model,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    seq_feature=\"tempo_seq\",\n",
    "    numeric_features=numeric_features,\n",
    "    text_features=text_features,\n",
    "    size=256,\n",
    "    prefix=\"train\",\n",
    ")\n",
    "\n",
    "dataset_test = SpotifyDataset(\n",
    "    frame=df_test,\n",
    "    model=bert_model,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    seq_feature=\"tempo_seq\",\n",
    "    numeric_features=numeric_features,\n",
    "    text_features=text_features,\n",
    "    size=100,\n",
    "    prefix=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train._prepare()\n",
    "dataset_test._prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_exists(path_out: str) -> None:\n",
    "    if os.path.exists(path_out):\n",
    "        return\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "\n",
    "def measure_time(fcn):\n",
    "    @wraps(fcn)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        start_time = time()\n",
    "        res = fcn(*args, **kwargs)\n",
    "        end_time = time()\n",
    "        print(\"Execution time: {:.4f}\".format(end_time - start_time))\n",
    "        return res\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "# @measure_time\n",
    "def run_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_function: Callable,\n",
    "    optimizer,\n",
    "    device: str = \"cuda\",\n",
    ") -> float:\n",
    "    average_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for anchor, positive, negative in data_loader:\n",
    "        # klasyfikator i funkcja kosztu\n",
    "        anchor_embedd = model(\n",
    "            anchor[0].to(device), anchor[1].to(device), anchor[2].to(device)\n",
    "        )\n",
    "        positive_embedd = model(\n",
    "            positive[0].to(device), positive[1].to(device), positive[2].to(device)\n",
    "        )\n",
    "        negative_embedd = model(\n",
    "            negative[0].to(device), negative[1].to(device), negative[2].to(device)\n",
    "        )\n",
    "        l = loss_function(anchor_embedd, positive_embedd, negative_embedd)\n",
    "\n",
    "        l.backward()\n",
    "\n",
    "        # optymalizacja\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        average_loss += l.item()\n",
    "        n_batches += 1\n",
    "    return average_loss / n_batches\n",
    "\n",
    "\n",
    "def run_validate(model, data_loader, loss_function, device=\"cuda\"):\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in data_loader:\n",
    "            i += 1\n",
    "            anchor_embedd = model(\n",
    "                anchor[0].to(device), anchor[1].to(device), anchor[2].to(device)\n",
    "            )\n",
    "            positive_embedd = model(\n",
    "                positive[0].to(device), positive[1].to(device), positive[2].to(device)\n",
    "            )\n",
    "            negative_embedd = model(\n",
    "                negative[0].to(device), negative[1].to(device), negative[2].to(device)\n",
    "            )\n",
    "            l = loss_function(anchor_embedd, positive_embedd, negative_embedd)\n",
    "            total_loss += l\n",
    "    return {\"loss\": total_loss / i}\n",
    "\n",
    "\n",
    "# @measure_time\n",
    "def fit(\n",
    "    model: Module,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    epochs: int,\n",
    "    writer: SummaryWriter,\n",
    "    device: str = \"cuda\",\n",
    "    patience: int = 10,\n",
    "    output_path: str = \"torch_logs/checkpoints/best\",\n",
    "    run_prefix: str = \"test\",\n",
    "    print_metrics: bool = True,\n",
    "):\n",
    "    min_val_loss = 1e10\n",
    "    current_patience = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_loader,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "        )\n",
    "        model.eval()\n",
    "        val_results = run_validate(\n",
    "            model=model,\n",
    "            data_loader=test_loader,\n",
    "            loss_function=loss_function,\n",
    "            device=device,\n",
    "        )\n",
    "        val_loss = val_results[\"loss\"]\n",
    "        writer.add_scalars(\n",
    "            main_tag=f\"{run_prefix} loss\",\n",
    "            tag_scalar_dict={\"train\": train_loss, \"val\": val_loss},\n",
    "            global_step=epoch + 1,\n",
    "        )\n",
    "        if print_metrics:\n",
    "            print(f\"Train loss: {train_loss} Val loss: {val_loss}\")\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            current_patience = 0\n",
    "            _ensure_exists(os.path.split(output_path)[0])\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                f=output_path + \"_\" + run_prefix,\n",
    "            )\n",
    "        else:\n",
    "            current_patience += 1\n",
    "\n",
    "        if current_patience >= patience:\n",
    "            break\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-db977a99e91a71c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-db977a99e91a71c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir = \"tensorboard_logs\"\n",
    "_ensure_exists(log_dir)\n",
    "\n",
    "writer_tensorboard = SummaryWriter(log_dir)\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir --port=6011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(Module):\n",
    "    def __init__(self, kwargs):\n",
    "        Module.__init__(self)\n",
    "        self.input_features_dim = kwargs.get(\"input_features_dim\", 11)\n",
    "        self.input_text_dim = kwargs.get(\"input_text_dim\", 2304)\n",
    "        self.hidden_text_dim = kwargs.get(\"hidden_text_dim\", 16)\n",
    "        self.input_lstm_dim = kwargs.get(\"input_lstm_dim\", 1)\n",
    "        self.hidden_lstm_dim = kwargs.get(\"hidden_lstm_dim\", 16)\n",
    "        self.num_layers_lstm = kwargs.get(\"num_layers_lstm\", 2)\n",
    "        self.hidden_dense_dim = kwargs.get(\"num_layers_lstm\", 32)\n",
    "        self.output_dim = kwargs.get(\"output_dim\", 16)\n",
    "        self.lstmSections = nn.LSTM(\n",
    "            input_size=self.input_lstm_dim,\n",
    "            hidden_size=self.hidden_lstm_dim,\n",
    "            num_layers=self.num_layers_lstm,\n",
    "            batch_first=True,\n",
    "        )  # lstm\n",
    "        self.fc_text = Linear(self.input_text_dim, 16)\n",
    "        merge_size = (\n",
    "            self.input_features_dim + self.hidden_text_dim + self.hidden_lstm_dim\n",
    "        )\n",
    "        self.fc1 = Linear(merge_size, self.hidden_dense_dim)\n",
    "        self.fc2 = Linear(self.hidden_dense_dim, self.output_dim)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "        embedds: torch.Tensor,\n",
    "        sections: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Output Text\n",
    "        embedds = self.relu(self.fc_text(embedds.clone().detach().requires_grad_(True)))\n",
    "\n",
    "        # Output LSTM\n",
    "        outputSections, (hnSections, cnSections) = self.lstmSections(\n",
    "            sections.clone().detach().requires_grad_(True)\n",
    "        )\n",
    "        hnSections = hnSections[-1].view(-1, self.hidden_lstm_dim)\n",
    "\n",
    "        # Output cat\n",
    "        output = self.relu(\n",
    "            self.fc1(\n",
    "                torch.cat(\n",
    "                    (\n",
    "                        embedds,\n",
    "                        hnSections,\n",
    "                        features.clone().detach().requires_grad_(True),\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                ).float()\n",
    "            )\n",
    "        )\n",
    "        output = self.fc2(output)\n",
    "        # Now process together\n",
    "        output = normalize(output, 2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with cuda\n",
      "------------------------------------------\n",
      "Starting training for model: EmbeddingModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:02<09:47,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: nan Val loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:05<09:40,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: nan Val loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:08<13:32,  4.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6b6ccb303ebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting training for model: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     fit(\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-88f8603fa496>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, epochs, writer, device, patience, output_path, run_prefix, print_metrics)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         train_loss = run_epoch(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-88f8603fa496>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, data_loader, loss_function, optimizer, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# klasyfikator i funkcja kosztu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         anchor_embedd = model(\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-af849cf509de>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_triplet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-af849cf509de>\u001b[0m in \u001b[0;36mget_random_triplet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 self.df[\n\u001b[1;32m    110\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_playlist\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnegative_playlist_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_uri\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manchor_song\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 ]\n\u001b[1;32m    113\u001b[0m             ):\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5501\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5502\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_embedding = EmbeddingModel({})\n",
    "\n",
    "models = {\"EmbeddingModel\": model_embedding}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models = {k: v.to(device) for (k, v) in models.items()}\n",
    "\n",
    "print(f\"Starting with {device}\")\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "LR = {\"EmbeddingModel\": 0.0001}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizers = {\n",
    "    \"EmbeddingModel\": torch.optim.Adam(\n",
    "        models[\"EmbeddingModel\"].parameters(), lr=LR[\"EmbeddingModel\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "loss_fcn = {\n",
    "    \"EmbeddingModel\": TripletMarginWithDistanceLoss(\n",
    "        margin=0.1, distance_function=PairwiseDistance()\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "to_train = [\"EmbeddingModel\"]\n",
    "\n",
    "for model_name in to_train:\n",
    "    time_stamp = datetime.now().strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Starting training for model: {model_name}\")\n",
    "    fit(\n",
    "        model=models[model_name],\n",
    "        train_loader=train_loader,\n",
    "        test_loader=val_loader,\n",
    "        loss_function=loss_fcn[model_name],\n",
    "        device=device,\n",
    "        optimizer=optimizers[model_name],\n",
    "        epochs=EPOCHS,\n",
    "        writer=writer_tensorboard,\n",
    "        output_path=\"torch_logs/checkpoints/best\",\n",
    "        patience=10,\n",
    "        run_prefix=model_name + \"_\" + time_stamp,\n",
    "        print_metrics=True,\n",
    "    )\n",
    "    checkpoint = torch.load(f\"torch_logs/checkpoints/best_{model_name}_{time_stamp}\")\n",
    "    models[model_name].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizers[model_name].load_state_dict(checkpoint[\"optimizer_state_dict\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spotify_scarper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
