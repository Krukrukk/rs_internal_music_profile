{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Callable, Any\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Linear,\n",
    "    ReLU,\n",
    "    TripletMarginLoss,\n",
    "    TripletMarginWithDistanceLoss,\n",
    "    PairwiseDistance,\n",
    "    CosineSimilarity\n",
    ")\n",
    "from torch.nn.functional import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('embeddings/pretrained/BertTokenizer/tokenizer_config.json',\n",
       " 'embeddings/pretrained/BertTokenizer/special_tokens_map.json',\n",
       " 'embeddings/pretrained/BertTokenizer/vocab.txt',\n",
       " 'embeddings/pretrained/BertTokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.save_pretrained(\"embeddings/pretrained/BertModel\")\n",
    "bert_tokenizer.save_pretrained(\"embeddings/pretrained/BertTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-de1dfe3a0de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bert_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     **bert_tokenizer(\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"przykładowy tekst w języku polskim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m ).last_hidden_state.mean(dim=1).shape\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "bert_model(\n",
    "    **bert_tokenizer(\n",
    "        \"przykładowy tekst w języku polskim\", return_tensors=\"pt\", padding=True\n",
    "    )\n",
    ").last_hidden_state.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset:\n",
    "\n",
    "- Positive example is a song paired with mean vector of a playlist songs, the song belongs to\n",
    "- Negative example is a song paired with mean vector of a playlist songs, the song doesn't belong to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_in_one_playlist_dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features: dict = {\n",
    "    \"duration_ms\": {\n",
    "        \"min_val\": 0,\n",
    "        \"max_val\": 6950000,\n",
    "        \"desc\": \"duration of a song in ms\",\n",
    "    },\n",
    "    \"danceability\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\",\n",
    "    },\n",
    "    \"energy\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\",\n",
    "    },\n",
    "    \"key\": {\n",
    "        \"min_val\": -1,\n",
    "        \"max_val\": 11,\n",
    "        \"desc\": \"The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\",\n",
    "    },\n",
    "    \"loudness\": {\n",
    "        \"min_val\": -60.0,\n",
    "        \"max_val\": 0.0,\n",
    "        \"desc\": \"The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\",\n",
    "    },\n",
    "    \"mode\": {\n",
    "        \"min_val\": 0,\n",
    "        \"max_val\": 1,\n",
    "        \"desc\": \"Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\",\n",
    "    },\n",
    "    \"speechiness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\",\n",
    "    },\n",
    "    \"acousticness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\",\n",
    "    },\n",
    "    \"instrumentalness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Predicts whether a track contains no vocals. 'Ooh' and 'aah' sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly 'vocal'. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\",\n",
    "    },\n",
    "    \"liveness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\",\n",
    "    },\n",
    "    \"valence\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\",\n",
    "    },\n",
    "}\n",
    "\n",
    "text_features = [\"track_name\", \"artist_name\", \"album_name\"]\n",
    "\n",
    "\n",
    "def preprocess_features(\n",
    "    df: pd.DataFrame, numeric_features: dict[str, dict[str, Any]], section_feature: str\n",
    ") -> pd.DataFrame:\n",
    "    stand_data = {}\n",
    "\n",
    "    # Process numeric features\n",
    "    for key in numeric_features:\n",
    "        # normalize data\n",
    "        # df[key] = df[key].fillna(0)\n",
    "        df[key] = (df[key] - numeric_features[key][\"min_val\"]) / (\n",
    "            numeric_features[key][\"max_val\"] - numeric_features[key][\"min_val\"]\n",
    "        )\n",
    "        stand_data[key] = {\n",
    "            \"mean\": df[key].mean(),\n",
    "            \"std\": df[key].std(),\n",
    "            \"max\": numeric_features[key][\"max_val\"],\n",
    "            \"min\": numeric_features[key][\"min_val\"],\n",
    "        }\n",
    "        # # standarize data\n",
    "        # df[key] = (df[key] - stand_data[key][\"mean\"]) / stand_data[key][\"std\"]\n",
    "\n",
    "    # Process sections data\n",
    "    seq_col = f\"{section_feature}_seq\"\n",
    "    df[seq_col] = df.analysis_sections.apply(\n",
    "        lambda section: torch.tensor(\n",
    "            [item[section_feature] for item in (section if section is not None else [])]\n",
    "        )\n",
    "    )\n",
    "    # Normalize/standarize data\n",
    "    all_tempo = torch.tensor([item for row in df[seq_col] for item in row])\n",
    "\n",
    "    max_tempo = all_tempo.max()\n",
    "    min_tempo = all_tempo.min()\n",
    "    all_tempo = (all_tempo - min_tempo) / (max_tempo - min_tempo)\n",
    "    stand_data[seq_col] = {\n",
    "        \"mean\": all_tempo.mean(),\n",
    "        \"std\": all_tempo.std(),\n",
    "        \"max\": max_tempo,\n",
    "        \"min\": min_tempo,\n",
    "    }\n",
    "\n",
    "    df[seq_col] = df[seq_col].apply(\n",
    "        lambda row: torch.tensor(\n",
    "            [((x - min_tempo) / (max_tempo - min_tempo)) * 2 - 1 for x in row]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # pad to 14\n",
    "    df[seq_col] = df[seq_col].apply(lambda x: x[:14])\n",
    "    df[seq_col] = df[seq_col].apply(\n",
    "        lambda x: torch.cat(\n",
    "            (\n",
    "                x,\n",
    "                torch.tensor([0] * (14 - len(x))),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return stand_data\n",
    "\n",
    "\n",
    "stand_data = preprocess_features(df, numeric_features, \"tempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration_ms': {'mean': 0.034333916002505675,\n",
       "  'std': 0.011708760542764075,\n",
       "  'max': 6950000,\n",
       "  'min': 0},\n",
       " 'danceability': {'mean': 0.6008363223754211,\n",
       "  'std': 0.1676360658383639,\n",
       "  'max': 1.0,\n",
       "  'min': 0.0},\n",
       " 'energy': {'mean': 0.6200577285783162,\n",
       "  'std': 0.21908584763805444,\n",
       "  'max': 1.0,\n",
       "  'min': 0.0},\n",
       " 'key': {'mean': 0.515858677661524,\n",
       "  'std': 0.301661135622213,\n",
       "  'max': 11,\n",
       "  'min': -1},\n",
       " 'loudness': {'mean': 0.8719418800245317,\n",
       "  'std': 0.06646245390106514,\n",
       "  'max': 0.0,\n",
       "  'min': -60.0},\n",
       " 'mode': {'mean': 0.6513954158538748,\n",
       "  'std': 0.47668099722459145,\n",
       "  'max': 1,\n",
       "  'min': 0},\n",
       " 'speechiness': {'mean': 0.09108276013034222,\n",
       "  'std': 0.09830546862857148,\n",
       "  'max': 1.0,\n",
       "  'min': 0.0},\n",
       " 'acousticness': {'mean': 0.2618604989430196,\n",
       "  'std': 0.294296959722126,\n",
       "  'max': 1.0,\n",
       "  'min': 0.0},\n",
       " 'instrumentalness': {'mean': 0.07918648350145194,\n",
       "  'std': 0.22214266693042017,\n",
       "  'max': 1.0,\n",
       "  'min': 0.0},\n",
       " 'liveness': {'mean': 0.18713169095805993,\n",
       "  'std': 0.15585710596509122,\n",
       "  'max': 1.0,\n",
       "  'min': 0.0},\n",
       " 'valence': {'mean': 0.4721523769728674,\n",
       "  'std': 0.24818187114749296,\n",
       "  'max': 1.0,\n",
       "  'min': 0.0},\n",
       " 'tempo_seq': {'mean': tensor(0.5026),\n",
       "  'std': tensor(0.1298),\n",
       "  'max': tensor(239.5450),\n",
       "  'min': tensor(0.)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "train_split = 0.8\n",
    "playlists = list(df.id_playlist.unique())\n",
    "total_len = len(playlists)\n",
    "random.shuffle(playlists)\n",
    "train_playlists = set(playlists[: int(0.8 * total_len)])\n",
    "test_playlists = set(playlists[len(train_playlists) :])\n",
    "\n",
    "df_train = df.query(\"id_playlist in @train_playlists\")\n",
    "df_test = df.query(\"id_playlist in @test_playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlist = df[df.id_playlist == 2]\n",
    "# torch.tensor(playlist[list(numeric_features.keys())].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_playlist', 'name', 'collaborative', 'pid', 'modified_at',\n",
       "       'num_tracks', 'num_albums', 'num_followers', 'num_edits', 'num_artists',\n",
       "       'pos', 'artist_name', 'track_uri', 'artist_uri', 'track_name',\n",
       "       'album_uri', 'duration_ms', 'album_name', 'danceability', 'energy',\n",
       "       'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
       "       'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature',\n",
       "       'num_samples', 'duration', 'offset_seconds', 'window_seconds',\n",
       "       'analysis_sample_rate', 'analysis_channels', 'end_of_fade_in',\n",
       "       'start_of_fade_out', 'tempo_confidence', 'time_signature_confidence',\n",
       "       'key_confidence', 'mode_confidence', 'analysis_sections', 'tempo_seq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_exists(path_out: str) -> None:\n",
    "    if os.path.exists(path_out):\n",
    "        return\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "\n",
    "class SpotifyDataset(Dataset):\n",
    "    \"\"\"Wrapper dataset that draws triplets from the Spotify dataset.\n",
    "\n",
    "    size: arbitrary 'length' of dataset, number of triplets to draw in one epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame: pd.DataFrame,\n",
    "        model: BertModel,\n",
    "        tokenizer: BertTokenizer,\n",
    "        seq_feature: str = \"\",\n",
    "        size: int = 10000,\n",
    "        numeric_features: dict = [],\n",
    "        text_features: list = [],\n",
    "        data_path: str = \"data/embedds\",\n",
    "        prefix: str = \"spotify\",\n",
    "        device: str = \"cuda\",\n",
    "        prepare: bool = False,\n",
    "    ):\n",
    "        super(SpotifyDataset, self).__init__()\n",
    "        self.df = frame\n",
    "        self.playlists = list(frame.id_playlist.unique())\n",
    "        self.numeric_features = numeric_features\n",
    "        self.text_features = text_features\n",
    "        self.seq_feature = seq_feature\n",
    "        self.size = size\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.numeric_keys = list(numeric_features.keys())\n",
    "        self.data_path = data_path\n",
    "        self.prefix = prefix\n",
    "        self.device = device\n",
    "        if prepare:\n",
    "            self._prepare()\n",
    "        self.playlist_bags = self.prepare_bags()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.get_random_triplet()\n",
    "\n",
    "    def _prepare(self):\n",
    "        _ensure_exists(self.data_path)\n",
    "        for _, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    self.data_path, f\"{self.prefix}_{row['track_uri']}.pckl\"\n",
    "                ).replace(\":\", \"_\"),\n",
    "                \"wb\",\n",
    "            ) as file:\n",
    "                pickle.dump(\n",
    "                    self.model(\n",
    "                        **self.tokenizer(\n",
    "                            list(row[self.text_features]),\n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=True,\n",
    "                        ).to(self.device)\n",
    "                    )\n",
    "                    .last_hidden_state.mean(dim=1)\n",
    "                    .flatten(),\n",
    "                    file,\n",
    "                )\n",
    "\n",
    "    def read_uri(self, uri):\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                self.data_path,\n",
    "                f\"{self.prefix}_{uri}.pckl\".replace(\":\", \"_\"),\n",
    "            ),\n",
    "            \"rb\",\n",
    "        ) as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "    def prepare_bags(self):\n",
    "        bags = {}\n",
    "        print(\"Prepairing playlists\")\n",
    "        for _id in tqdm(self.playlists):\n",
    "            playlist = self.df[self.df.id_playlist == _id]\n",
    "            playlist_embedd = torch.stack(\n",
    "                [self.read_uri(row[\"track_uri\"]) for i, row in playlist.iterrows()]\n",
    "            ).mean(axis=0)\n",
    "\n",
    "            bags[_id] = {\n",
    "                \"data\": playlist,\n",
    "                \"features\": torch.tensor(playlist[self.numeric_keys].mean()).float(),\n",
    "                \"embedds\": playlist_embedd.float(),\n",
    "                \"seq\": torch.stack(list(playlist[self.seq_feature]))\n",
    "                .float()\n",
    "                .mean(axis=0)\n",
    "                .reshape(-1, 1),\n",
    "            }\n",
    "        return bags\n",
    "\n",
    "    def get_random_triplet(self, track_uri: str = None, to_cpu: bool = False):\n",
    "        if track_uri:\n",
    "            try:\n",
    "                anchor_song = self.df[self.df[\"track_uri\"] == track_uri].iloc[0]\n",
    "            except:\n",
    "                print(track_uri)\n",
    "                raise\n",
    "        else:\n",
    "            # get a random song and its playlist\n",
    "            anchor_song = self.df.sample(1).iloc[0]\n",
    "        anchor_playlist_id: int = anchor_song.id_playlist\n",
    "\n",
    "        # positive eanchor_songxample\n",
    "        positive_playlist_id: int = anchor_playlist_id\n",
    "\n",
    "        # negative example -> get a playlist song does't belong to. Stupid solution\n",
    "        while True:\n",
    "            negative_playlist_id = random.choice(self.playlists)\n",
    "            # check if it is a different playlist\n",
    "            if negative_playlist_id == anchor_playlist_id:\n",
    "                continue  # find another oneds\n",
    "            # check if it doesn't have our song\n",
    "            if len(\n",
    "                self.df[\n",
    "                    (self.df.id_playlist == negative_playlist_id)\n",
    "                    & (self.df.track_uri == anchor_song.track_uri)\n",
    "                ]\n",
    "            ):\n",
    "                continue  # find another one\n",
    "            break\n",
    "\n",
    "        # positive and negative playlist data\n",
    "        positive_playlist = self.playlist_bags[positive_playlist_id][\"data\"]\n",
    "        negative_playlist = self.playlist_bags[negative_playlist_id][\"data\"]\n",
    "\n",
    "        # get anchor numeric features\n",
    "        anchor_song_features = torch.tensor(anchor_song[self.numeric_keys]).float()\n",
    "\n",
    "        # get positive and negative playlist numeric features\n",
    "        positive_playlist_features = self.playlist_bags[positive_playlist_id][\n",
    "            \"features\"\n",
    "        ]\n",
    "        negative_playlist_features = self.playlist_bags[negative_playlist_id][\n",
    "            \"features\"\n",
    "        ]\n",
    "\n",
    "        # get anchor text features\n",
    "        anchor_song_embedds = self.read_uri(anchor_song[\"track_uri\"]).float()\n",
    "\n",
    "        # get positive and negative playlist text features\n",
    "        positive_playlist_embedds = self.playlist_bags[positive_playlist_id][\"embedds\"]\n",
    "        negative_playlist_embedds = self.playlist_bags[negative_playlist_id][\"embedds\"]\n",
    "\n",
    "        # get anchor sequence features\n",
    "        anchor_song_seq = anchor_song[self.seq_feature].reshape(-1, 1).float()\n",
    "\n",
    "        # get positive/negative playlist seq features\n",
    "        positive_playlist_seq = self.playlist_bags[positive_playlist_id][\"seq\"]\n",
    "        negative_playlist_seq = self.playlist_bags[negative_playlist_id][\"seq\"]\n",
    "\n",
    "        if to_cpu is True:\n",
    "            device = \"cpu\"\n",
    "        else:\n",
    "            device = self.device\n",
    "\n",
    "        return (\n",
    "            (\n",
    "                anchor_song_features.to(device),\n",
    "                anchor_song_embedds.to(device),\n",
    "                anchor_song_seq.to(device),\n",
    "            ),\n",
    "            (\n",
    "                positive_playlist_features.to(device),\n",
    "                positive_playlist_embedds.to(device),\n",
    "                positive_playlist_seq.to(device),\n",
    "            ),\n",
    "            (\n",
    "                negative_playlist_features.to(device),\n",
    "                negative_playlist_embedds.to(device),\n",
    "                negative_playlist_seq.to(device),\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    def get_anchor(self, track_uri: str = None, to_cpu: bool = False):\n",
    "        anchor_song = self.df[self.df[\"track_uri\"] == track_uri].iloc[0]\n",
    "        anchor_playlist_id: int = anchor_song.id_playlist\n",
    "\n",
    "        # get anchor numeric features\n",
    "        anchor_song_features = torch.tensor(anchor_song[self.numeric_keys]).float()\n",
    "\n",
    "\n",
    "        # get anchor text features\n",
    "        anchor_song_embedds = self.read_uri(anchor_song[\"track_uri\"]).float()\n",
    "\n",
    "        # get anchor sequence features\n",
    "        anchor_song_seq = anchor_song[self.seq_feature].reshape(-1, 1).float()\n",
    "\n",
    "        if to_cpu is True:\n",
    "            device = \"cpu\"\n",
    "        else:\n",
    "            device = self.device\n",
    "\n",
    "        return (\n",
    "            (\n",
    "                anchor_song_features.to(device),\n",
    "                anchor_song_embedds.to(device),\n",
    "                anchor_song_seq.to(device),\n",
    "            )\n",
    "        ), (anchor_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing playlists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2674/2674 [00:23<00:00, 113.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing playlists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 669/669 [00:05<00:00, 115.93it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = SpotifyDataset(\n",
    "    frame=df_train,\n",
    "    model=bert_model,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    seq_feature=\"tempo_seq\",\n",
    "    numeric_features=numeric_features,\n",
    "    text_features=text_features,\n",
    "    size=10000,\n",
    "    prefix=\"train\",\n",
    "    prepare=False,\n",
    ")\n",
    "\n",
    "dataset_test = SpotifyDataset(\n",
    "    frame=df_test,\n",
    "    model=bert_model,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    seq_feature=\"tempo_seq\",\n",
    "    numeric_features=numeric_features,\n",
    "    text_features=text_features,\n",
    "    size=100,\n",
    "    prefix=\"test\",\n",
    "    prepare=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(fcn):\n",
    "    @wraps(fcn)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        start_time = time()\n",
    "        res = fcn(*args, **kwargs)\n",
    "        end_time = time()\n",
    "        print(\"Execution time: {:.4f}\".format(end_time - start_time))\n",
    "        return res\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "# @measure_time\n",
    "def run_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_function: Callable,\n",
    "    optimizer,\n",
    "    device: str = \"cuda\",\n",
    ") -> float:\n",
    "    average_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for (anchor, positive, negative) in data_loader:\n",
    "        # klasyfikator i funkcja kosztu\n",
    "        anchor_embedd = model(\n",
    "            anchor[0].to(device), anchor[1].to(device), anchor[2].to(device)\n",
    "        )\n",
    "        positive_embedd = model(\n",
    "            positive[0].to(device), positive[1].to(device), positive[2].to(device)\n",
    "        )\n",
    "        negative_embedd = model(\n",
    "            negative[0].to(device), negative[1].to(device), negative[2].to(device)\n",
    "        )\n",
    "        l = loss_function(anchor_embedd, positive_embedd, negative_embedd)\n",
    "\n",
    "        l.backward()\n",
    "\n",
    "        # optymalizacja\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        average_loss += l.item()\n",
    "        n_batches += 1\n",
    "    return average_loss / n_batches\n",
    "\n",
    "\n",
    "def run_validate(model, data_loader, loss_function, device=\"cuda\"):\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for (anchor, positive, negative) in data_loader:\n",
    "            i += 1\n",
    "            anchor_embedd = model(\n",
    "                anchor[0].to(device), anchor[1].to(device), anchor[2].to(device)\n",
    "            )\n",
    "            positive_embedd = model(\n",
    "                positive[0].to(device), positive[1].to(device), positive[2].to(device)\n",
    "            )\n",
    "            negative_embedd = model(\n",
    "                negative[0].to(device), negative[1].to(device), negative[2].to(device)\n",
    "            )\n",
    "            l = loss_function(anchor_embedd, positive_embedd, negative_embedd)\n",
    "            total_loss += l\n",
    "    return {\"loss\": total_loss / i}\n",
    "\n",
    "\n",
    "# @measure_time\n",
    "def fit(\n",
    "    model: Module,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    epochs: int,\n",
    "    writer: SummaryWriter,\n",
    "    device: str = \"cuda\",\n",
    "    patience: int = 10,\n",
    "    output_path: str = \"torch_logs/checkpoints/best\",\n",
    "    run_prefix: str = \"test\",\n",
    "    print_metrics: bool = True,\n",
    "):\n",
    "    min_val_loss = 1e10\n",
    "    current_patience = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_loader,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "        )\n",
    "        model.eval()\n",
    "        val_results = run_validate(\n",
    "            model=model,\n",
    "            data_loader=test_loader,\n",
    "            loss_function=loss_function,\n",
    "            device=device,\n",
    "        )\n",
    "        val_loss = val_results[\"loss\"]\n",
    "        writer.add_scalars(\n",
    "            main_tag=f\"{run_prefix} loss\",\n",
    "            tag_scalar_dict={\"train\": train_loss, \"val\": val_loss},\n",
    "            global_step=epoch + 1,\n",
    "        )\n",
    "        if print_metrics:\n",
    "            print(f\"Train loss: {train_loss} Val loss: {val_loss}\")\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            current_patience = 0\n",
    "            _ensure_exists(os.path.split(output_path)[0])\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                f=output_path + \"_\" + run_prefix,\n",
    "            )\n",
    "        else:\n",
    "            current_patience += 1\n",
    "\n",
    "        if current_patience >= patience:\n",
    "            break\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir = \"tensorboard_logs\"\n",
    "_ensure_exists(log_dir)\n",
    "\n",
    "writer_tensorboard = SummaryWriter(log_dir)\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir --port=6011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(Module):\n",
    "    def __init__(self, kwargs):\n",
    "        Module.__init__(self)\n",
    "        self.input_features_dim = kwargs.get(\"input_features_dim\", 11)\n",
    "        self.input_text_dim = kwargs.get(\"input_text_dim\", 2304)\n",
    "        self.hidden_text_dim = kwargs.get(\"hidden_text_dim\", 16)\n",
    "        self.input_lstm_dim = kwargs.get(\"input_lstm_dim\", 1)\n",
    "        self.hidden_lstm_dim = kwargs.get(\"hidden_lstm_dim\", 16)\n",
    "        self.num_layers_lstm = kwargs.get(\"num_layers_lstm\", 2)\n",
    "        self.hidden_dense_dim = kwargs.get(\"num_layers_lstm\", 32)\n",
    "        self.output_dim = kwargs.get(\"output_dim\", 16)\n",
    "        self.lstmSections = nn.LSTM(\n",
    "            input_size=self.input_lstm_dim,\n",
    "            hidden_size=self.hidden_lstm_dim,\n",
    "            num_layers=self.num_layers_lstm,\n",
    "            batch_first=True,\n",
    "        )  # lstm\n",
    "        self.fc_text = Linear(self.input_text_dim, 16)\n",
    "        merge_size = (\n",
    "            self.input_features_dim + self.hidden_text_dim + self.hidden_lstm_dim\n",
    "        )\n",
    "        self.fc1 = Linear(merge_size, self.hidden_dense_dim)\n",
    "        self.fc2 = Linear(self.hidden_dense_dim, self.output_dim)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "        embedds: torch.Tensor,\n",
    "        sections: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Output Text\n",
    "        embedds = self.relu(self.fc_text(embedds.clone().detach().requires_grad_(True)))\n",
    "\n",
    "        # Output LSTM\n",
    "        outputSections, (hnSections, cnSections) = self.lstmSections(\n",
    "            sections.clone().detach().requires_grad_(True)\n",
    "        )\n",
    "        hnSections = hnSections[-1].view(-1, self.hidden_lstm_dim)\n",
    "\n",
    "        # Output cat\n",
    "        output = self.relu(\n",
    "            self.fc1(\n",
    "                torch.cat(\n",
    "                    (\n",
    "                        embedds,\n",
    "                        hnSections,\n",
    "                        features.clone().detach().requires_grad_(True),\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                ).float()\n",
    "            )\n",
    "        )\n",
    "        output = self.fc2(output)\n",
    "        # Now process together\n",
    "        output = normalize(output, 2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with cuda\n",
      "------------------------------------------\n",
      "Starting training for model: EmbeddingModelCosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [01:13<4:02:40, 73.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10409471727907657 Val loss: 0.10291509330272675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [02:26<4:00:58, 73.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10190884098410606 Val loss: 0.10195048898458481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [03:39<3:59:54, 73.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10110696367919444 Val loss: 0.10106384754180908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [04:53<4:00:18, 73.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10075028762221336 Val loss: 0.10087358206510544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [06:05<3:57:45, 73.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10057304874062538 Val loss: 0.10065126419067383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [07:21<3:58:41, 73.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10045204348862172 Val loss: 0.10045995563268661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [08:34<3:56:54, 73.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10039854645729065 Val loss: 0.10049018263816833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [09:47<3:55:26, 73.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10033045969903469 Val loss: 0.10025554150342941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [11:01<3:54:42, 73.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10027565211057662 Val loss: 0.10027941316366196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [12:16<3:54:38, 74.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10023749209940433 Val loss: 0.10019498318433762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [13:29<3:52:24, 73.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10022377409040928 Val loss: 0.10028062760829926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [14:43<3:51:08, 73.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10018972158432007 Val loss: 0.10018149763345718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 13/200 [15:57<3:49:44, 73.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10016975998878479 Val loss: 0.10014872997999191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [17:09<3:47:01, 73.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.100166841968894 Val loss: 0.10017968714237213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/200 [18:20<3:44:13, 72.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10014724619686603 Val loss: 0.1001376286149025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [19:31<3:41:24, 72.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10013211853802204 Val loss: 0.10012941062450409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [20:43<3:39:48, 72.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10012853741645814 Val loss: 0.10019101947546005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [21:53<3:36:49, 71.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10011186935007572 Val loss: 0.10011507570743561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [23:04<3:34:37, 71.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10010519064962864 Val loss: 0.10010001808404922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [24:14<3:32:33, 70.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10009394735097885 Val loss: 0.10006275027990341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [25:26<3:32:37, 71.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10008501745760441 Val loss: 0.1001538410782814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 22/200 [26:41<3:34:44, 72.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10008628517389298 Val loss: 0.10011796653270721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [27:53<3:33:21, 72.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10007667355239391 Val loss: 0.10008400678634644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 24/200 [29:04<3:30:57, 71.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10006949342787266 Val loss: 0.10013096779584885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 25/200 [30:15<3:28:33, 71.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10006714016199111 Val loss: 0.10014507174491882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 26/200 [31:25<3:26:35, 71.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10006060525774955 Val loss: 0.10005339980125427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 27/200 [32:36<3:24:37, 70.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10005681216716766 Val loss: 0.10005435347557068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [33:46<3:22:58, 70.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10005283877253532 Val loss: 0.10013274103403091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 29/200 [34:57<3:21:43, 70.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10005181729793548 Val loss: 0.10008374601602554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [36:07<3:20:15, 70.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10005009546875954 Val loss: 0.1000744104385376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [37:18<3:19:08, 70.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10005112625658512 Val loss: 0.10011939704418182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 32/200 [38:29<3:18:01, 70.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1000465627759695 Val loss: 0.10003972798585892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 33/200 [39:39<3:16:51, 70.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1000414527952671 Val loss: 0.10007276386022568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/200 [40:50<3:15:37, 70.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10004152581095696 Val loss: 0.10004635900259018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 35/200 [42:01<3:14:25, 70.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1000351656228304 Val loss: 0.10005035996437073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 36/200 [43:14<3:15:24, 71.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1000314425677061 Val loss: 0.10009683668613434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 37/200 [44:28<3:16:16, 72.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1000351656228304 Val loss: 0.10005872696638107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 38/200 [45:42<3:16:26, 72.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10002976693212987 Val loss: 0.10001841932535172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39/200 [46:54<3:14:54, 72.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10002956837415695 Val loss: 0.10002893954515457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [48:06<3:12:40, 72.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10002698712050914 Val loss: 0.10002604871988297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [49:18<3:11:09, 72.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10002655759453774 Val loss: 0.1000305563211441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 42/200 [50:30<3:09:46, 72.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10002415105700493 Val loss: 0.10003393888473511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 43/200 [51:42<3:08:35, 72.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10002022609114647 Val loss: 0.10001363605260849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 44/200 [52:54<3:07:22, 72.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10001758113503456 Val loss: 0.10001439601182938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 45/200 [54:06<3:06:03, 72.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10001654922962189 Val loss: 0.10003311187028885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 46/200 [55:18<3:05:06, 72.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10000997334718705 Val loss: 0.10004004836082458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 47/200 [56:31<3:04:12, 72.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10000836662948132 Val loss: 0.09999635815620422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 48/200 [57:43<3:03:05, 72.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09999947845935822 Val loss: 0.09999372065067291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/200 [58:57<3:03:19, 72.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09999760612845421 Val loss: 0.09999609738588333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [1:00:10<3:02:28, 72.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09998940899968148 Val loss: 0.09989220649003983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 51/200 [1:01:24<3:02:05, 73.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09996733851730824 Val loss: 0.09994678944349289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 52/200 [1:02:38<3:01:21, 73.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09992575794458389 Val loss: 0.10010343044996262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 53/200 [1:03:51<2:59:24, 73.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09986348487436772 Val loss: 0.09951908141374588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 54/200 [1:05:04<2:57:46, 73.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09970543757081032 Val loss: 0.09872541576623917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 55/200 [1:06:17<2:56:34, 73.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0991179183125496 Val loss: 0.0946933701634407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 56/200 [1:07:31<2:56:17, 73.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0979144211858511 Val loss: 0.09964293986558914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 57/200 [1:08:46<2:56:24, 74.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09735613763332367 Val loss: 0.09608739614486694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 58/200 [1:10:03<2:56:48, 74.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09722314961254597 Val loss: 0.09612216800451279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 59/200 [1:11:18<2:55:48, 74.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09726430699229241 Val loss: 0.094892717897892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 60/200 [1:12:33<2:54:32, 74.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09646637588739396 Val loss: 0.09675885736942291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 61/200 [1:13:46<2:52:23, 74.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09713325276970863 Val loss: 0.09723033756017685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 62/200 [1:15:00<2:50:55, 74.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09590908214449882 Val loss: 0.10244843363761902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 63/200 [1:16:13<2:48:31, 73.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09609788618981838 Val loss: 0.09650662541389465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 64/200 [1:17:28<2:47:54, 74.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09571351185441017 Val loss: 0.09345302730798721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 65/200 [1:18:42<2:46:42, 74.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0948974385857582 Val loss: 0.09237977117300034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 66/200 [1:19:56<2:45:26, 74.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09357935115695 Val loss: 0.09491458535194397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 67/200 [1:21:08<2:42:49, 73.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0920678809285164 Val loss: 0.09172277897596359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [1:22:21<2:41:11, 73.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0910867266356945 Val loss: 0.08962676674127579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 69/200 [1:23:35<2:40:34, 73.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0888154398649931 Val loss: 0.08992360532283783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 70/200 [1:24:48<2:38:56, 73.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08756212592124939 Val loss: 0.08599342405796051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 71/200 [1:25:59<2:36:12, 72.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08582325428724288 Val loss: 0.08141154795885086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 72/200 [1:27:10<2:34:13, 72.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08348294384777546 Val loss: 0.09379773586988449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 73/200 [1:28:22<2:32:58, 72.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08317503184080124 Val loss: 0.08297254145145416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 74/200 [1:29:37<2:33:02, 72.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08270604126155376 Val loss: 0.08259501308202744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 75/200 [1:30:51<2:32:51, 73.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08181416504085064 Val loss: 0.08992074429988861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 76/200 [1:32:07<2:32:52, 73.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08022334016859531 Val loss: 0.07657494395971298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 77/200 [1:33:20<2:31:34, 73.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08014401085674763 Val loss: 0.0876498892903328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 78/200 [1:34:34<2:30:15, 73.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0791904978454113 Val loss: 0.08292856067419052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 79/200 [1:35:47<2:28:27, 73.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07857236750423909 Val loss: 0.07809840887784958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80/200 [1:37:01<2:27:14, 73.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07738675251603126 Val loss: 0.08303812146186829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [1:38:13<2:25:18, 73.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0769883755594492 Val loss: 0.0825415626168251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 82/200 [1:39:27<2:24:16, 73.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07723356187343597 Val loss: 0.06980647146701813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 83/200 [1:40:41<2:23:44, 73.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07580381855368615 Val loss: 0.07926183193922043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 84/200 [1:41:54<2:21:37, 73.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07450390122830867 Val loss: 0.07782445847988129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 85/200 [1:43:06<2:19:46, 72.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07372906282544137 Val loss: 0.05950700491666794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 86/200 [1:44:18<2:18:05, 72.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07428744472563267 Val loss: 0.09563163667917252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 87/200 [1:45:32<2:17:33, 73.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07445858046412468 Val loss: 0.07133326679468155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 88/200 [1:46:46<2:17:06, 73.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07272207364439964 Val loss: 0.09172207862138748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 89/200 [1:48:00<2:16:06, 73.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07364913485944272 Val loss: 0.07509603351354599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 90/200 [1:49:15<2:15:36, 73.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0721798524260521 Val loss: 0.08909673243761063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 91/200 [1:50:27<2:13:29, 73.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07281517125666141 Val loss: 0.08864570409059525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 92/200 [1:51:43<2:13:48, 74.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07085649892687798 Val loss: 0.06880342960357666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 93/200 [1:52:58<2:12:40, 74.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07094623260200024 Val loss: 0.09253158420324326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 94/200 [1:54:11<2:10:56, 74.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0702742662280798 Val loss: 0.08815746009349823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 94/200 [1:55:26<2:10:10, 73.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06986254192888737 Val loss: 0.08472307026386261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_embedding = EmbeddingModel({})\n",
    "\n",
    "models = {\"EmbeddingModel\": model_embedding, \"EmbeddingModelCosine\": model_embedding}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models = {k: v.to(device) for (k, v) in models.items()}\n",
    "\n",
    "print(f\"Starting with {device}\")\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 512\n",
    "LR = {\"EmbeddingModel\": 0.0001, \"EmbeddingModelCosine\": 0.0001}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizers = {\n",
    "    \"EmbeddingModel\": torch.optim.Adam(\n",
    "        models[\"EmbeddingModel\"].parameters(), lr=LR[\"EmbeddingModel\"]\n",
    "    ),\n",
    "    \"EmbeddingModelCosine\": torch.optim.Adam(\n",
    "        models[\"EmbeddingModelCosine\"].parameters(), lr=LR[\"EmbeddingModelCosine\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "loss_fcn = {\n",
    "    \"EmbeddingModel\": TripletMarginWithDistanceLoss(\n",
    "        margin=0.1, distance_function=PairwiseDistance()\n",
    "    ),\n",
    "    \"EmbeddingModelCosine\": TripletMarginWithDistanceLoss(\n",
    "        margin=0.1, distance_function=CosineSimilarity()\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "to_train = [\"EmbeddingModelCosine\"]\n",
    "\n",
    "for model_name in to_train:\n",
    "    time_stamp = datetime.now().strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Starting training for model: {model_name}\")\n",
    "    fit(\n",
    "        model=models[model_name],\n",
    "        train_loader=train_loader,\n",
    "        test_loader=val_loader,\n",
    "        loss_function=loss_fcn[model_name],\n",
    "        device=device,\n",
    "        optimizer=optimizers[model_name],\n",
    "        epochs=EPOCHS,\n",
    "        writer=writer_tensorboard,\n",
    "        output_path=\"torch_logs/checkpoints/best\",\n",
    "        patience=10,\n",
    "        run_prefix=model_name + \"_\" + time_stamp,\n",
    "        print_metrics=True,\n",
    "    )\n",
    "    checkpoint = torch.load(f\"torch_logs/checkpoints/best_{model_name}_{time_stamp}\")\n",
    "    models[model_name].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizers[model_name].load_state_dict(checkpoint[\"optimizer_state_dict\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"embeddings/pretrained/EmbeddingModel\")\n",
    "models[\"EmbeddingModel\"].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizers[\"EmbeddingModel\"].load_state_dict(checkpoint[\"optimizer_state_dict\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"embeddings/pretrained/EmbeddingModelCosine\")\n",
    "models[\"EmbeddingModelCosine\"].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizers[\"EmbeddingModelCosine\"].load_state_dict(checkpoint[\"optimizer_state_dict\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uris_train = df_train[\"track_uri\"]\n",
    "# songs_train = [dataset_train.get_anchor(uri, True) for uri in tqdm(uris_train)]\n",
    "\n",
    "# with open(\"songs_train.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(songs_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"songs_train.pkl\", \"rb\") as file:\n",
    "    songs_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uris_test = df_test[\"track_uri\"]\n",
    "# songs_test = [dataset_test.get_anchor(uri, True) for uri in tqdm(uris_test)]\n",
    "\n",
    "# with open(\"songs_test.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(songs_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"songs_test.pkl\", \"rb\") as file:\n",
    "    songs_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144052/144052 [00:01<00:00, 128060.52it/s]\n"
     ]
    }
   ],
   "source": [
    "songs_train_meta = [\n",
    "    {\n",
    "        \"title\": song[1][\"track_name\"],\n",
    "        \"artist_name\": song[1][\"artist_name\"],\n",
    "        \"track_uri\": song[1][\"track_uri\"],\n",
    "        \"data\": song[0],\n",
    "    }\n",
    "    for song in tqdm(songs_train)\n",
    "]\n",
    "\n",
    "\n",
    "with open(\"songs_train_meta.pkl\", \"wb\") as file:\n",
    "    pickle.dump(songs_train_meta, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"songs_train_meta.pkl\", \"rb\") as file:\n",
    "    songs_train_meta = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36396/36396 [00:00<00:00, 112510.30it/s]\n"
     ]
    }
   ],
   "source": [
    "songs_test_meta = [\n",
    "    {\n",
    "        \"title\": song[1][\"track_name\"],\n",
    "        \"artist_name\": song[1][\"artist_name\"],\n",
    "        \"track_uri\": song[1][\"track_uri\"],\n",
    "        \"data\": song[0],\n",
    "    }\n",
    "    for song in tqdm(songs_test)\n",
    "]\n",
    "\n",
    "\n",
    "with open(\"songs_test_meta.pkl\", \"wb\") as file:\n",
    "    pickle.dump(songs_test_meta, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"songs_test_meta.pkl\", \"rb\") as file:\n",
    "    songs_test_meta = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144052/144052 [01:25<00:00, 1694.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, song in enumerate(tqdm(songs_train_meta)):\n",
    "    with torch.no_grad():\n",
    "        songs_train_meta[i][\"embedding\"] = models[\"EmbeddingModelCosine\"](\n",
    "            song[\"data\"][0].to(\"cuda\").unsqueeze(dim=0),\n",
    "            song[\"data\"][1].to(\"cuda\").unsqueeze(dim=0),\n",
    "            song[\"data\"][2].to(\"cuda\").unsqueeze(dim=0),\n",
    "        )\n",
    "        song[\"data\"][0].to(\"cpu\")\n",
    "        song[\"data\"][1].to(\"cpu\")\n",
    "        song[\"data\"][2].to(\"cpu\")\n",
    "    \n",
    "    \n",
    "with open(\"songs_train_meta_embedding_cosine.pkl\", \"wb\") as file:\n",
    "    pickle.dump(songs_train_meta, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36396/36396 [00:22<00:00, 1650.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, song in enumerate(tqdm(songs_test_meta)):\n",
    "    with torch.no_grad():\n",
    "        songs_test_meta[i][\"embedding\"] = models[\"EmbeddingModelCosine\"](\n",
    "            song[\"data\"][0].to(\"cuda\").unsqueeze(dim=0),\n",
    "            song[\"data\"][1].to(\"cuda\").unsqueeze(dim=0),\n",
    "            song[\"data\"][2].to(\"cuda\").unsqueeze(dim=0),\n",
    "        )\n",
    "        song[\"data\"][0].to(\"cpu\")\n",
    "        song[\"data\"][1].to(\"cpu\")\n",
    "        song[\"data\"][2].to(\"cpu\")\n",
    "\n",
    "with open(\"songs_test_meta_embedding_cosine.pkl\", \"wb\") as file:\n",
    "    pickle.dump(songs_test_meta, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_all_meta_embedding_cosine = songs_train_meta + songs_test_meta\n",
    "with open(\"songs_all_meta_embedding_cosine.pkl\", \"wb\") as file:\n",
    "    pickle.dump(songs_all_meta_embedding_cosine, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"track_uri\"] == \"spotify:track:45poGZbUrcBgMEed0xtV5W\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_to_test_1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)[[\"artist_name\", \"track_name\", \"track_uri\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spotify_scarper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
