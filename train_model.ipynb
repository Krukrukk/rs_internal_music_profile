{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Callable, Any\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Linear,\n",
    "    ReLU,\n",
    "    TripletMarginLoss,\n",
    "    TripletMarginWithDistanceLoss,\n",
    "    PairwiseDistance,\n",
    ")\n",
    "from torch.nn.functional import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model(\n",
    "    **bert_tokenizer(\n",
    "        \"przykładowy tekst w języku polskim\", return_tensors=\"pt\", padding=True\n",
    "    )\n",
    ").last_hidden_state.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset:\n",
    "\n",
    "- Positive example is a song paired with mean vector of a playlist songs, the song belongs to\n",
    "- Negative example is a song paired with mean vector of a playlist songs, the song doesn't belong to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_in_one_playlist_dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = [\"track_name\", \"artist_name\"]\n",
    "numeric_features: dict = {\n",
    "    \"duration_ms\": {\n",
    "        \"min_val\": 0,\n",
    "        \"max_val\": 6950000,\n",
    "        \"desc\": \"duration of a song in ms\",\n",
    "    },\n",
    "    \"danceability\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\",\n",
    "    },\n",
    "    \"energy\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\",\n",
    "    },\n",
    "    \"key\": {\n",
    "        \"min_val\": -1,\n",
    "        \"max_val\": 11,\n",
    "        \"desc\": \"The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\",\n",
    "    },\n",
    "    \"loudness\": {\n",
    "        \"min_val\": -60.0,\n",
    "        \"max_val\": 0.0,\n",
    "        \"desc\": \"The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\",\n",
    "    },\n",
    "    \"mode\": {\n",
    "        \"min_val\": 0,\n",
    "        \"max_val\": 1,\n",
    "        \"desc\": \"Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\",\n",
    "    },\n",
    "    \"speechiness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\",\n",
    "    },\n",
    "    \"acousticness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\",\n",
    "    },\n",
    "    \"instrumentalness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Predicts whether a track contains no vocals. 'Ooh' and 'aah' sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly 'vocal'. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\",\n",
    "    },\n",
    "    \"liveness\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\",\n",
    "    },\n",
    "    \"valence\": {\n",
    "        \"min_val\": 0.0,\n",
    "        \"max_val\": 1.0,\n",
    "        \"desc\": \"A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\",\n",
    "    },\n",
    "}\n",
    "\n",
    "text_features = [\"track_name\", \"artist_name\", \"album_name\"]\n",
    "\n",
    "\n",
    "def preprocess_features(\n",
    "    df: pd.DataFrame, numeric_features: dict[str, dict[str, Any]], section_feature: str\n",
    ") -> pd.DataFrame:\n",
    "    stand_data = {}\n",
    "\n",
    "    # Process numeric features\n",
    "    for key in numeric_features:\n",
    "        # normalize data\n",
    "        # df[key] = df[key].fillna(0)\n",
    "        df[key] = (df[key] - numeric_features[key][\"min_val\"]) / (\n",
    "            numeric_features[key][\"max_val\"] - numeric_features[key][\"min_val\"]\n",
    "        )\n",
    "        stand_data[key] = {\n",
    "            \"mean\": df[key].mean(),\n",
    "            \"std\": df[key].std(),\n",
    "            \"max\": numeric_features[key][\"max_val\"],\n",
    "            \"min\": numeric_features[key][\"min_val\"],\n",
    "        }\n",
    "        # # standarize data\n",
    "        # df[key] = (df[key] - stand_data[key][\"mean\"]) / stand_data[key][\"std\"]\n",
    "\n",
    "    # Process sections data\n",
    "    seq_col = f\"{section_feature}_seq\"\n",
    "    df[seq_col] = df.analysis_sections.apply(\n",
    "        lambda section: torch.tensor(\n",
    "            [item[section_feature] for item in (section if section is not None else [])]\n",
    "        )\n",
    "    )\n",
    "    # Normalize/standarize data\n",
    "    all_tempo = torch.tensor([item for row in df[seq_col] for item in row])\n",
    "\n",
    "    max_tempo = all_tempo.max()\n",
    "    min_tempo = all_tempo.min()\n",
    "    all_tempo = (all_tempo - min_tempo) / (max_tempo - min_tempo)\n",
    "    stand_data[seq_col] = {\n",
    "        \"mean\": all_tempo.mean(),\n",
    "        \"std\": all_tempo.std(),\n",
    "        \"max\": max_tempo,\n",
    "        \"min\": min_tempo,\n",
    "    }\n",
    "\n",
    "    df[seq_col] = df[seq_col].apply(\n",
    "        lambda row: torch.tensor(\n",
    "            [((x - min_tempo) / (max_tempo - min_tempo)) * 2 - 1 for x in row]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # pad to 14\n",
    "    df[seq_col] = df[seq_col].apply(lambda x: x[:14])\n",
    "    df[seq_col] = df[seq_col].apply(\n",
    "        lambda x: torch.cat(\n",
    "            (\n",
    "                x,\n",
    "                torch.tensor([0] * (14 - len(x))),\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return stand_data\n",
    "\n",
    "\n",
    "stand_data = preprocess_features(df, numeric_features, \"tempo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "train_split = 0.8\n",
    "playlists = list(df.id_playlist.unique())\n",
    "total_len = len(playlists)\n",
    "random.shuffle(playlists)\n",
    "train_playlists = set(playlists[: int(0.8 * total_len)])\n",
    "test_playlists = set(playlists[len(train_playlists) :])\n",
    "\n",
    "df_train = df.query(\"id_playlist in @train_playlists\")\n",
    "df_test = df.query(\"id_playlist in @test_playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlist = df[df.id_playlist == 2]\n",
    "# torch.tensor(playlist[list(numeric_features.keys())].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_playlist', 'name', 'collaborative', 'pid', 'modified_at',\n",
       "       'num_tracks', 'num_albums', 'num_followers', 'num_edits', 'num_artists',\n",
       "       'pos', 'artist_name', 'track_uri', 'artist_uri', 'track_name',\n",
       "       'album_uri', 'duration_ms', 'album_name', 'danceability', 'energy',\n",
       "       'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
       "       'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature',\n",
       "       'num_samples', 'duration', 'offset_seconds', 'window_seconds',\n",
       "       'analysis_sample_rate', 'analysis_channels', 'end_of_fade_in',\n",
       "       'start_of_fade_out', 'tempo_confidence', 'time_signature_confidence',\n",
       "       'key_confidence', 'mode_confidence', 'analysis_sections', 'tempo_seq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_exists(path_out: str) -> None:\n",
    "    if os.path.exists(path_out):\n",
    "        return\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "\n",
    "class SpotifyDataset(Dataset):\n",
    "    \"\"\"Wrapper dataset that draws triplets from the Spotify dataset.\n",
    "\n",
    "    size: arbitrary 'length' of dataset, number of triplets to draw in one epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame: pd.DataFrame,\n",
    "        model: BertModel,\n",
    "        tokenizer: BertTokenizer,\n",
    "        seq_feature: str = \"\",\n",
    "        size: int = 10000,\n",
    "        numeric_features: dict = [],\n",
    "        text_features: list = [],\n",
    "        data_path: str = \"data/embedds\",\n",
    "        prefix: str = \"spotify\",\n",
    "        device: str = \"cuda\",\n",
    "        prepare: bool = False,\n",
    "    ):\n",
    "        super(SpotifyDataset, self).__init__()\n",
    "        self.df = frame\n",
    "        self.playlists = list(frame.id_playlist.unique())\n",
    "        self.numeric_features = numeric_features\n",
    "        self.text_features = text_features\n",
    "        self.seq_feature = seq_feature\n",
    "        self.size = size\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.numeric_keys = list(numeric_features.keys())\n",
    "        self.data_path = data_path\n",
    "        self.prefix = prefix\n",
    "        self.device = device\n",
    "        if prepare:\n",
    "            self._prepare()\n",
    "        self.playlist_bags = self.prepare_bags()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.get_random_triplet()\n",
    "\n",
    "    def _prepare(self):\n",
    "        _ensure_exists(self.data_path)\n",
    "        for _, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    self.data_path, f\"{self.prefix}_{row['track_uri']}.pckl\"\n",
    "                ).replace(\":\", \"_\"),\n",
    "                \"wb\",\n",
    "            ) as file:\n",
    "                pickle.dump(\n",
    "                    self.model(\n",
    "                        **self.tokenizer(\n",
    "                            list(row[self.text_features]),\n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=True,\n",
    "                        ).to(self.device)\n",
    "                    )\n",
    "                    .last_hidden_state.mean(dim=1)\n",
    "                    .flatten(),\n",
    "                    file,\n",
    "                )\n",
    "\n",
    "    def read_uri(self, uri):\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                self.data_path,\n",
    "                f\"{self.prefix}_{uri}.pckl\".replace(\":\", \"_\"),\n",
    "            ),\n",
    "            \"rb\",\n",
    "        ) as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "    def prepare_bags(self):\n",
    "        bags = {}\n",
    "        print(\"Prepairing playlists\")\n",
    "        for _id in tqdm(self.playlists):\n",
    "            playlist = self.df[self.df.id_playlist == _id]\n",
    "            playlist_embedd = torch.stack(\n",
    "                [self.read_uri(row[\"track_uri\"]) for i, row in playlist.iterrows()]\n",
    "            ).mean(axis=0)\n",
    "\n",
    "            bags[_id] = {\n",
    "                \"data\": playlist,\n",
    "                \"features\": torch.tensor(playlist[self.numeric_keys].mean()).float(),\n",
    "                \"embedds\": playlist_embedd.float(),\n",
    "                \"seq\": torch.stack(list(playlist[self.seq_feature]))\n",
    "                .float()\n",
    "                .mean(axis=0)\n",
    "                .reshape(-1, 1),\n",
    "            }\n",
    "        return bags\n",
    "\n",
    "    def get_random_triplet(self, track_uri: str = None):\n",
    "        if track_uri:\n",
    "            anchor_song = self.df[df[\"track_uri\"] == track_uri].iloc[0]\n",
    "        else:\n",
    "            # get a random song and its playlist\n",
    "            anchor_song = self.df.sample(1).iloc[0]\n",
    "        anchor_playlist_id: int = anchor_song.id_playlist\n",
    "\n",
    "        # positive eanchor_songxample\n",
    "        positive_playlist_id: int = anchor_playlist_id\n",
    "\n",
    "        # negative example -> get a playlist song does't belong to. Stupid solution\n",
    "        while True:\n",
    "            negative_playlist_id = random.choice(self.playlists)\n",
    "            # check if it is a different playlist\n",
    "            if negative_playlist_id == anchor_playlist_id:\n",
    "                continue  # find another oneds\n",
    "            # check if it doesn't have our song\n",
    "            if len(\n",
    "                self.df[\n",
    "                    (self.df.id_playlist == negative_playlist_id)\n",
    "                    & (self.df.track_uri == anchor_song.track_uri)\n",
    "                ]\n",
    "            ):\n",
    "                continue  # find another one\n",
    "            break\n",
    "\n",
    "        # positive and negative playlist data\n",
    "        positive_playlist = self.playlist_bags[positive_playlist_id][\"data\"]\n",
    "        negative_playlist = self.playlist_bags[negative_playlist_id][\"data\"]\n",
    "\n",
    "        # get anchor numeric features\n",
    "        anchor_song_features = torch.tensor(anchor_song[self.numeric_keys]).float()\n",
    "\n",
    "        # get positive and negative playlist numeric features\n",
    "        positive_playlist_features = self.playlist_bags[positive_playlist_id][\n",
    "            \"features\"\n",
    "        ]\n",
    "        negative_playlist_features = self.playlist_bags[negative_playlist_id][\n",
    "            \"features\"\n",
    "        ]\n",
    "\n",
    "        # get anchor text features\n",
    "        anchor_song_embedds = self.read_uri(anchor_song[\"track_uri\"]).float()\n",
    "\n",
    "        # get positive and negative playlist text features\n",
    "        positive_playlist_embedds = self.playlist_bags[positive_playlist_id][\"embedds\"]\n",
    "        negative_playlist_embedds = self.playlist_bags[negative_playlist_id][\"embedds\"]\n",
    "\n",
    "        # get anchor sequence features\n",
    "        anchor_song_seq = anchor_song[self.seq_feature].reshape(-1, 1).float()\n",
    "\n",
    "        # get positive/negative playlist seq features\n",
    "        positive_playlist_seq = self.playlist_bags[positive_playlist_id][\"seq\"]\n",
    "        negative_playlist_seq = self.playlist_bags[negative_playlist_id][\"seq\"]\n",
    "        return (\n",
    "            (\n",
    "                anchor_song_features,\n",
    "                anchor_song_embedds,\n",
    "                anchor_song_seq,\n",
    "            ),\n",
    "            (\n",
    "                positive_playlist_features,\n",
    "                positive_playlist_embedds,\n",
    "                positive_playlist_seq,\n",
    "            ),\n",
    "            (\n",
    "                negative_playlist_features,\n",
    "                negative_playlist_embedds,\n",
    "                negative_playlist_seq,\n",
    "            ),\n",
    "        ), (anchor_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing playlists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2674/2674 [00:26<00:00, 100.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing playlists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 669/669 [00:06<00:00, 104.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = SpotifyDataset(\n",
    "    frame=df_train,\n",
    "    model=bert_model,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    seq_feature=\"tempo_seq\",\n",
    "    numeric_features=numeric_features,\n",
    "    text_features=text_features,\n",
    "    size=10000,\n",
    "    prefix=\"train\",\n",
    "    prepare=False,\n",
    ")\n",
    "\n",
    "dataset_test = SpotifyDataset(\n",
    "    frame=df_test,\n",
    "    model=bert_model,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    seq_feature=\"tempo_seq\",\n",
    "    numeric_features=numeric_features,\n",
    "    text_features=text_features,\n",
    "    size=100,\n",
    "    prefix=\"test\",\n",
    "    prepare=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(fcn):\n",
    "    @wraps(fcn)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        start_time = time()\n",
    "        res = fcn(*args, **kwargs)\n",
    "        end_time = time()\n",
    "        print(\"Execution time: {:.4f}\".format(end_time - start_time))\n",
    "        return res\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "# @measure_time\n",
    "def run_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_function: Callable,\n",
    "    optimizer,\n",
    "    device: str = \"cuda\",\n",
    ") -> float:\n",
    "    average_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for (anchor, positive, negative), anchor_song in data_loader:\n",
    "        # klasyfikator i funkcja kosztu\n",
    "        anchor_embedd = model(\n",
    "            anchor[0].to(device), anchor[1].to(device), anchor[2].to(device)\n",
    "        )\n",
    "        positive_embedd = model(\n",
    "            positive[0].to(device), positive[1].to(device), positive[2].to(device)\n",
    "        )\n",
    "        negative_embedd = model(\n",
    "            negative[0].to(device), negative[1].to(device), negative[2].to(device)\n",
    "        )\n",
    "        l = loss_function(anchor_embedd, positive_embedd, negative_embedd)\n",
    "\n",
    "        l.backward()\n",
    "\n",
    "        # optymalizacja\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        average_loss += l.item()\n",
    "        n_batches += 1\n",
    "    return average_loss / n_batches\n",
    "\n",
    "\n",
    "def run_validate(model, data_loader, loss_function, device=\"cuda\"):\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for (anchor, positive, negative), anchor_song in data_loader:\n",
    "            i += 1\n",
    "            anchor_embedd = model(\n",
    "                anchor[0].to(device), anchor[1].to(device), anchor[2].to(device)\n",
    "            )\n",
    "            positive_embedd = model(\n",
    "                positive[0].to(device), positive[1].to(device), positive[2].to(device)\n",
    "            )\n",
    "            negative_embedd = model(\n",
    "                negative[0].to(device), negative[1].to(device), negative[2].to(device)\n",
    "            )\n",
    "            l = loss_function(anchor_embedd, positive_embedd, negative_embedd)\n",
    "            total_loss += l\n",
    "    return {\"loss\": total_loss / i}\n",
    "\n",
    "\n",
    "# @measure_time\n",
    "def fit(\n",
    "    model: Module,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    epochs: int,\n",
    "    writer: SummaryWriter,\n",
    "    device: str = \"cuda\",\n",
    "    patience: int = 10,\n",
    "    output_path: str = \"torch_logs/checkpoints/best\",\n",
    "    run_prefix: str = \"test\",\n",
    "    print_metrics: bool = True,\n",
    "):\n",
    "    min_val_loss = 1e10\n",
    "    current_patience = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = run_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_loader,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "        )\n",
    "        model.eval()\n",
    "        val_results = run_validate(\n",
    "            model=model,\n",
    "            data_loader=test_loader,\n",
    "            loss_function=loss_function,\n",
    "            device=device,\n",
    "        )\n",
    "        val_loss = val_results[\"loss\"]\n",
    "        writer.add_scalars(\n",
    "            main_tag=f\"{run_prefix} loss\",\n",
    "            tag_scalar_dict={\"train\": train_loss, \"val\": val_loss},\n",
    "            global_step=epoch + 1,\n",
    "        )\n",
    "        if print_metrics:\n",
    "            print(f\"Train loss: {train_loss} Val loss: {val_loss}\")\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            current_patience = 0\n",
    "            _ensure_exists(os.path.split(output_path)[0])\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                f=output_path + \"_\" + run_prefix,\n",
    "            )\n",
    "        else:\n",
    "            current_patience += 1\n",
    "\n",
    "        if current_patience >= patience:\n",
    "            break\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 92454), started 6:37:39 ago. (Use '!kill 92454' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-db977a99e91a71c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-db977a99e91a71c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir = \"tensorboard_logs\"\n",
    "_ensure_exists(log_dir)\n",
    "\n",
    "writer_tensorboard = SummaryWriter(log_dir)\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir --port=6011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(Module):\n",
    "    def __init__(self, kwargs):\n",
    "        Module.__init__(self)\n",
    "        self.input_features_dim = kwargs.get(\"input_features_dim\", 11)\n",
    "        self.input_text_dim = kwargs.get(\"input_text_dim\", 2304)\n",
    "        self.hidden_text_dim = kwargs.get(\"hidden_text_dim\", 16)\n",
    "        self.input_lstm_dim = kwargs.get(\"input_lstm_dim\", 1)\n",
    "        self.hidden_lstm_dim = kwargs.get(\"hidden_lstm_dim\", 16)\n",
    "        self.num_layers_lstm = kwargs.get(\"num_layers_lstm\", 2)\n",
    "        self.hidden_dense_dim = kwargs.get(\"num_layers_lstm\", 32)\n",
    "        self.output_dim = kwargs.get(\"output_dim\", 16)\n",
    "        self.lstmSections = nn.LSTM(\n",
    "            input_size=self.input_lstm_dim,\n",
    "            hidden_size=self.hidden_lstm_dim,\n",
    "            num_layers=self.num_layers_lstm,\n",
    "            batch_first=True,\n",
    "        )  # lstm\n",
    "        self.fc_text = Linear(self.input_text_dim, 16)\n",
    "        merge_size = (\n",
    "            self.input_features_dim + self.hidden_text_dim + self.hidden_lstm_dim\n",
    "        )\n",
    "        self.fc1 = Linear(merge_size, self.hidden_dense_dim)\n",
    "        self.fc2 = Linear(self.hidden_dense_dim, self.output_dim)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "        embedds: torch.Tensor,\n",
    "        sections: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Output Text\n",
    "        embedds = self.relu(self.fc_text(embedds.clone().detach().requires_grad_(True)))\n",
    "\n",
    "        # Output LSTM\n",
    "        outputSections, (hnSections, cnSections) = self.lstmSections(\n",
    "            sections.clone().detach().requires_grad_(True)\n",
    "        )\n",
    "        hnSections = hnSections[-1].view(-1, self.hidden_lstm_dim)\n",
    "\n",
    "        # Output cat\n",
    "        output = self.relu(\n",
    "            self.fc1(\n",
    "                torch.cat(\n",
    "                    (\n",
    "                        embedds,\n",
    "                        hnSections,\n",
    "                        features.clone().detach().requires_grad_(True),\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                ).float()\n",
    "            )\n",
    "        )\n",
    "        output = self.fc2(output)\n",
    "        # Now process together\n",
    "        output = normalize(output, 2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with cuda\n"
     ]
    }
   ],
   "source": [
    "model_embedding = EmbeddingModel({})\n",
    "\n",
    "models = {\"EmbeddingModel\": model_embedding}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models = {k: v.to(device) for (k, v) in models.items()}\n",
    "\n",
    "print(f\"Starting with {device}\")\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 512\n",
    "LR = {\"EmbeddingModel\": 0.0001}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizers = {\n",
    "    \"EmbeddingModel\": torch.optim.Adam(\n",
    "        models[\"EmbeddingModel\"].parameters(), lr=LR[\"EmbeddingModel\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "loss_fcn = {\n",
    "    \"EmbeddingModel\": TripletMarginWithDistanceLoss(\n",
    "        margin=0.1, distance_function=PairwiseDistance()\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "to_train = []\n",
    "\n",
    "for model_name in to_train:\n",
    "    time_stamp = datetime.now().strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Starting training for model: {model_name}\")\n",
    "    fit(\n",
    "        model=models[model_name],\n",
    "        train_loader=train_loader,\n",
    "        test_loader=val_loader,\n",
    "        loss_function=loss_fcn[model_name],\n",
    "        device=device,\n",
    "        optimizer=optimizers[model_name],\n",
    "        epochs=EPOCHS,\n",
    "        writer=writer_tensorboard,\n",
    "        output_path=\"torch_logs/checkpoints/best\",\n",
    "        patience=10,\n",
    "        run_prefix=model_name + \"_\" + time_stamp,\n",
    "        print_metrics=True,\n",
    "    )\n",
    "    checkpoint = torch.load(f\"torch_logs/checkpoints/best_{model_name}_{time_stamp}\")\n",
    "    models[model_name].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizers[model_name].load_state_dict(checkpoint[\"optimizer_state_dict\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"torch_logs/checkpoints/best_EmbeddingModel_14_01_22_03_30_39\")\n",
    "models[\"EmbeddingModel\"].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizers[\"EmbeddingModel\"].load_state_dict(checkpoint[\"optimizer_state_dict\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-a9eb3c8f2543>:104: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  anchor_song = self.df[df[\"track_uri\"] == track_uri].iloc[0]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Piano guys\n",
    "song_1_uri = \"spotify:track:5gMEMaqm74DwNKxZ0RfWM6\"\n",
    "# Bowie\n",
    "song_2_uri = \"spotify:track:7IBfmCrfaR3SLVemWhsucQ\"\n",
    "# 50 Cent\n",
    "song_3_uri = \"spotify:track:7iL6o9tox1zgHpKUfh9vuC\"\n",
    "\n",
    "\n",
    "uris = [\n",
    "    \"spotify:track:5gMEMaqm74DwNKxZ0RfWM6\",\n",
    "    \"spotify:track:7IBfmCrfaR3SLVemWhsucQ\",\n",
    "    \"spotify:track:7iL6o9tox1zgHpKUfh9vuC\",\n",
    "]\n",
    "\n",
    "songs = [dataset_test.get_random_triplet(uri) for uri in uris]\n",
    "\n",
    "songs_meta = [\n",
    "    {\n",
    "        \"title\": song[1][\"track_name\"],\n",
    "        \"artist_name\": song[1][\"artist_name\"],\n",
    "        \"track_uri\": song[1][\"track_uri\"],\n",
    "        \"data\": song[0][0],\n",
    "    }\n",
    "    for song in songs\n",
    "]\n",
    "songs_meta\n",
    "\n",
    "\n",
    "# song_to_test_1 = dataset_test.get_random_triplet(songs_meta)[0][0]\n",
    "# song_to_test_2 = dataset_test.get_random_triplet(song_2_uri)[0][0]\n",
    "# song_to_test_3 = dataset_test.get_random_triplet(song_3_uri)[0][0]\n",
    "\n",
    "\n",
    "for i, song in enumerate(songs_meta):\n",
    "    songs_meta[i][\"embedding\"] = models[\"EmbeddingModel\"](\n",
    "        song[\"data\"][0].to(\"cuda\").unsqueeze(dim=0),\n",
    "        song[\"data\"][1].to(\"cuda\").unsqueeze(dim=0),\n",
    "        song[\"data\"][2].to(\"cuda\").unsqueeze(dim=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': \"Beethoven's 5 Secrets\",\n",
       "  'artist_name': 'The Piano Guys',\n",
       "  'track_uri': 'spotify:track:5gMEMaqm74DwNKxZ0RfWM6',\n",
       "  'data': (tensor([0.0446, 0.3020, 0.5150, 0.2500, 0.8450, 1.0000, 0.0414, 0.7950, 0.9040,\n",
       "           0.0790, 0.1110]),\n",
       "   tensor([-0.1538,  0.2166, -0.1599,  ...,  0.0519,  0.0887, -0.3725],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   tensor([[0.2378],\n",
       "           [0.2650],\n",
       "           [0.2528],\n",
       "           [0.2526],\n",
       "           [0.2516],\n",
       "           [0.3037],\n",
       "           [0.2518],\n",
       "           [0.2485],\n",
       "           [0.2777],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000]])),\n",
       "  'embedding': tensor([[ 0.2185, -0.0160, -0.1314,  0.1455, -0.2276,  0.1177, -0.4430,  0.2883,\n",
       "           -0.0800, -0.0154,  0.1174, -0.3140, -0.5185,  0.1040,  0.3464,  0.2232]],\n",
       "         device='cuda:0', grad_fn=<DivBackward0>)},\n",
       " {'title': 'Space Oddity - 1999 Remastered Version',\n",
       "  'artist_name': 'David Bowie',\n",
       "  'track_uri': 'spotify:track:7IBfmCrfaR3SLVemWhsucQ',\n",
       "  'data': (tensor([4.5516e-02, 3.1000e-01, 4.2300e-01, 8.3333e-02, 7.8497e-01, 1.0000e+00,\n",
       "           3.2400e-02, 1.6900e-02, 1.3500e-05, 3.6500e-01, 4.5700e-01]),\n",
       "   tensor([-1.0562e-01,  2.5457e-01,  2.1503e-01,  ...,  8.4549e-05,\n",
       "            1.7072e-01, -3.5187e-01], device='cuda:0', requires_grad=True),\n",
       "   tensor([[0.1077],\n",
       "           [0.1205],\n",
       "           [0.1160],\n",
       "           [0.1289],\n",
       "           [0.1365],\n",
       "           [0.1398],\n",
       "           [0.1929],\n",
       "           [0.1563],\n",
       "           [0.1489],\n",
       "           [0.1510],\n",
       "           [0.1860],\n",
       "           [0.1956],\n",
       "           [0.2036],\n",
       "           [0.0000]])),\n",
       "  'embedding': tensor([[ 0.2236,  0.0049,  0.0975,  0.1754, -0.1935,  0.1409, -0.5918,  0.3230,\n",
       "            0.0075, -0.0720, -0.2188, -0.2283, -0.2775,  0.0723,  0.3470,  0.3000]],\n",
       "         device='cuda:0', grad_fn=<DivBackward0>)},\n",
       " {'title': 'In Da Club',\n",
       "  'artist_name': '50 Cent',\n",
       "  'track_uri': 'spotify:track:7iL6o9tox1zgHpKUfh9vuC',\n",
       "  'data': (tensor([0.0278, 0.8990, 0.7130, 0.5833, 0.9541, 0.0000, 0.3660, 0.2550, 0.0000,\n",
       "           0.0708, 0.7770]),\n",
       "   tensor([-0.2527,  0.1517,  0.3713,  ...,  0.2758,  0.5221, -0.4437],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   tensor([[-0.2468],\n",
       "           [-0.2472],\n",
       "           [-0.2478],\n",
       "           [-0.2459],\n",
       "           [-0.2477],\n",
       "           [-0.2479],\n",
       "           [-0.2483],\n",
       "           [-0.2479],\n",
       "           [-0.2475],\n",
       "           [ 0.0000],\n",
       "           [ 0.0000],\n",
       "           [ 0.0000],\n",
       "           [ 0.0000],\n",
       "           [ 0.0000]])),\n",
       "  'embedding': tensor([[-0.1974, -0.3515,  0.3069,  0.2177, -0.1678, -0.1327, -0.5146,  0.2990,\n",
       "            0.0076,  0.0111, -0.3582, -0.0765,  0.0241, -0.0263,  0.3046,  0.2599]],\n",
       "         device='cuda:0', grad_fn=<DivBackward0>)}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_to_test_1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)[[\"artist_name\", \"track_name\", \"track_uri\"]]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "spotify_scarper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
